{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Missing and Duplicate Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this mission, we'll explore each of these options in detail and learn when to use them. We'll work with the 2015, 2016, and 2017 World Happiness Reports again - more specifically, we'll combine them and clean missing values as we start to define a more complete data cleaning workflow.\n",
    "<br>\n",
    "<br>\n",
    "First is to go through the information for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "happiness2015 = pd.read_csv(\"World_Happiness_2015.csv\")\n",
    "happiness2016 = pd.read_csv(\"World_Happiness_2016.csv\")\n",
    "happiness2017 = pd.read_csv(\"World_Happiness_2017.csv\")\n",
    "\n",
    "happiness2015['Year'] = 2015\n",
    "happiness2016['Year'] = 2016\n",
    "happiness2017['Year'] = 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(158, 13)\n",
      "(157, 14)\n",
      "(155, 13)\n"
     ]
    }
   ],
   "source": [
    "shape_2015 = happiness2015.shape\n",
    "shape_2016 = happiness2016.shape\n",
    "shape_2017 = happiness2017.shape\n",
    "\n",
    "print(shape_2015)\n",
    "print(shape_2016)\n",
    "print(shape_2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying Missing Values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the dataframes were updated so that each contains the same countries, even if the happiness score, happiness rank, etc. were missing. However, that also means that each likely contains missing values.\n",
    "<br>\n",
    "<br>\n",
    "However, pandas will not automatically identify values such as n/a, -, or -- as NaN or None, but they may also indicate data is missing.\n",
    "See here for more information on how to use the pd.read_csv() function to read those values in as NaN.\n",
    "<br>\n",
    "<br>\n",
    "Once we ensure that all missing values were read in correctly, we can use the Series.isnull() method to identify rows with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country                          0\n",
      "Region                           0\n",
      "Happiness Rank                   0\n",
      "Happiness Score                  0\n",
      "Lower Confidence Interval        0\n",
      "Upper Confidence Interval        0\n",
      "Economy (GDP per Capita)         0\n",
      "Family                           0\n",
      "Health (Life Expectancy)         0\n",
      "Freedom                          0\n",
      "Trust (Government Corruption)    0\n",
      "Generosity                       0\n",
      "Dystopia Residual                0\n",
      "Year                             0\n",
      "dtype: int64\n",
      "Country                          0\n",
      "Happiness.Rank                   0\n",
      "Happiness.Score                  0\n",
      "Whisker.high                     0\n",
      "Whisker.low                      0\n",
      "Economy..GDP.per.Capita.         0\n",
      "Family                           0\n",
      "Health..Life.Expectancy.         0\n",
      "Freedom                          0\n",
      "Generosity                       0\n",
      "Trust..Government.Corruption.    0\n",
      "Dystopia.Residual                0\n",
      "Year                             0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "missing_2016 = happiness2016.isnull().sum()\n",
    "missing_2017 = happiness2017.isnull().sum()\n",
    "print(missing_2016)\n",
    "print(missing_2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correcting Data Cleaning Errors that results in missing values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's good to check for missing values before transforming data to make sure we don't unintentionally introduce missing values.\n",
    "<br>\n",
    "<br>\n",
    "If we do introduce missing values after transforming data, we'll have to determine if the data is really missing or if it's the result of some kind of error. As we progress through this mission, we'll use the following workflow to clean our missing values, starting with checking for errors:\n",
    "- Check for errors in data cleaning/transformation\n",
    "- Use data from additional sources to fill missing values\n",
    "- Drop row/column\n",
    "- Fill missing values with reasonable estimates computed from the available data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Country  Dystopia Residual  Dystopia.Residual  \\\n",
      "0  Switzerland            2.51738                NaN   \n",
      "1      Iceland            2.70201                NaN   \n",
      "2      Denmark            2.49204                NaN   \n",
      "3       Norway            2.46531                NaN   \n",
      "4       Canada            2.45176                NaN   \n",
      "\n",
      "   Economy (GDP per Capita)  Economy..GDP.per.Capita.   Family  Freedom  \\\n",
      "0                   1.39651                       NaN  1.34951  0.66557   \n",
      "1                   1.30232                       NaN  1.40223  0.62877   \n",
      "2                   1.32548                       NaN  1.36058  0.64938   \n",
      "3                   1.45900                       NaN  1.33095  0.66973   \n",
      "4                   1.32629                       NaN  1.32261  0.63297   \n",
      "\n",
      "   Generosity  Happiness Rank  Happiness Score  ...  Health..Life.Expectancy.  \\\n",
      "0     0.29678             1.0            7.587  ...                       NaN   \n",
      "1     0.43630             2.0            7.561  ...                       NaN   \n",
      "2     0.34139             3.0            7.527  ...                       NaN   \n",
      "3     0.34699             4.0            7.522  ...                       NaN   \n",
      "4     0.45811             5.0            7.427  ...                       NaN   \n",
      "\n",
      "   Lower Confidence Interval          Region  Standard Error  \\\n",
      "0                        NaN  Western Europe         0.03411   \n",
      "1                        NaN  Western Europe         0.04884   \n",
      "2                        NaN  Western Europe         0.03328   \n",
      "3                        NaN  Western Europe         0.03880   \n",
      "4                        NaN   North America         0.03553   \n",
      "\n",
      "   Trust (Government Corruption) Trust..Government.Corruption.  \\\n",
      "0                        0.41978                           NaN   \n",
      "1                        0.14145                           NaN   \n",
      "2                        0.48357                           NaN   \n",
      "3                        0.36503                           NaN   \n",
      "4                        0.32957                           NaN   \n",
      "\n",
      "   Upper Confidence Interval  Whisker.high  Whisker.low  Year  \n",
      "0                        NaN           NaN          NaN  2015  \n",
      "1                        NaN           NaN          NaN  2015  \n",
      "2                        NaN           NaN          NaN  2015  \n",
      "3                        NaN           NaN          NaN  2015  \n",
      "4                        NaN           NaN          NaN  2015  \n",
      "\n",
      "[5 rows x 23 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pakhl\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "combined = pd.concat([happiness2015, happiness2016, happiness2017], ignore_index=True)\n",
    "print(combined.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Country                            0\n",
       "Dystopia Residual                155\n",
       "Dystopia.Residual                315\n",
       "Economy (GDP per Capita)         155\n",
       "Economy..GDP.per.Capita.         315\n",
       "Family                             0\n",
       "Freedom                            0\n",
       "Generosity                         0\n",
       "Happiness Rank                   155\n",
       "Happiness Score                  155\n",
       "Happiness.Rank                   315\n",
       "Happiness.Score                  315\n",
       "Health (Life Expectancy)         155\n",
       "Health..Life.Expectancy.         315\n",
       "Lower Confidence Interval        313\n",
       "Region                           155\n",
       "Standard Error                   312\n",
       "Trust (Government Corruption)    155\n",
       "Trust..Government.Corruption.    315\n",
       "Upper Confidence Interval        313\n",
       "Whisker.high                     315\n",
       "Whisker.low                      315\n",
       "Year                               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we'll update the column names to make them uniform and combine the dataframes again with the replace function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pakhl\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "COUNTRY                          0\n",
       "DYSTOPIA RESIDUAL                0\n",
       "ECONOMY GDP PER CAPITA           0\n",
       "FAMILY                           0\n",
       "FREEDOM                          0\n",
       "GENEROSITY                       0\n",
       "HAPPINESS RANK                   0\n",
       "HAPPINESS SCORE                  0\n",
       "HEALTH LIFE EXPECTANCY           0\n",
       "LOWER CONFIDENCE INTERVAL      313\n",
       "REGION                         155\n",
       "STANDARD ERROR                 312\n",
       "TRUST GOVERNMENT CORRUPTION      0\n",
       "UPPER CONFIDENCE INTERVAL      313\n",
       "WHISKER HIGH                   315\n",
       "WHISKER LOW                    315\n",
       "YEAR                             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "happiness2017.columns = happiness2017.columns.str.replace('.', ' ').str.replace('\\s+', ' ').str.strip().str.upper()\n",
    "happiness2015.columns = happiness2015.columns.str.replace(r'[\\(\\)]', '').str.strip().str.upper()\n",
    "happiness2016.columns = happiness2016.columns.str.replace(r'[\\(\\)]', '').str.strip().str.upper()\n",
    "\n",
    "combined = pd.concat([happiness2015, happiness2016, happiness2017], ignore_index=True)\n",
    "missing = combined.isnull().sum()\n",
    "missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can pick some missing values for further investigation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "155"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regions_2017 = combined[combined['YEAR']==2017]['REGION']\n",
    "missing = regions_2017.isnull().sum()\n",
    "missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Data From Additional Source to Fill in Missing Value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can pick some missing values for further investigation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "155"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regions_2017 = combined[combined['YEAR']==2017]['REGION']\n",
    "missing = regions_2017.isnull().sum()\n",
    "missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We confirmed that the REGION column is missing from the 2017 data. Since we need the regions to analyze our data, let's turn our attention there next.\n",
    "<br>\n",
    "<br>\n",
    "Before we drop or replace any values, let's first see if there's a way we can use other available data to correct the values.\n",
    "- Check for errors in data cleaning/transformation.\n",
    "- Use data from additional sources to fill missing values.\n",
    "- Drop row/column\n",
    "- Fill missing values with reasonable estimates computed from the available data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall once more that each year contains the same countries. Since the regions are fixed values - the region a country was assigned to in 2015 or 2016 won't change - we should be able to assign the 2015 or 2016 region to the 2017 row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       COUNTRY          REGION\n",
      "0  Switzerland  Western Europe\n",
      "1      Iceland  Western Europe\n",
      "2      Denmark  Western Europe\n",
      "3       Norway  Western Europe\n",
      "4       Canada   North America\n"
     ]
    }
   ],
   "source": [
    "regions = combined[(combined['YEAR']==2015)|(combined['YEAR']==2016)][['COUNTRY', 'REGION']]\n",
    "print(regions.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "COUNTRY                          0\n",
       "DYSTOPIA RESIDUAL                0\n",
       "ECONOMY GDP PER CAPITA           0\n",
       "FAMILY                           0\n",
       "FREEDOM                          0\n",
       "GENEROSITY                       0\n",
       "HAPPINESS RANK                   0\n",
       "HAPPINESS SCORE                  0\n",
       "HEALTH LIFE EXPECTANCY           0\n",
       "LOWER CONFIDENCE INTERVAL      610\n",
       "STANDARD ERROR                 609\n",
       "TRUST GOVERNMENT CORRUPTION      0\n",
       "UPPER CONFIDENCE INTERVAL      610\n",
       "WHISKER HIGH                   617\n",
       "WHISKER LOW                    617\n",
       "YEAR                             0\n",
       "REGION_y                         2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined = pd.merge(left=combined, right=regions, on='COUNTRY', how='left')\n",
    "combined = combined.drop('REGION_x', axis=1)\n",
    "missing = combined.isnull().sum()\n",
    "missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying Duplicate Value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we decide how to handle the rest of our missing values, let's first check our dataframe for duplicate rows.\n",
    "<br>\n",
    "<br>\n",
    "We'll use the DataFrame.duplicated() method to check for duplicate values. If no parameters are specified, the method will check for any rows in which all columns have the same values.\n",
    "<br>\n",
    "<br>\n",
    "However, one thing to keep in mind is that the df.duplicated() method will only look for exact matches, so if the capitalization for country names isn't exactly the same, they won't be identified as duplicates.\n",
    "<br>\n",
    "<br>\n",
    "Since we should only have one country for each year, we can be a little more thorough by defining rows with ONLY the same country and year as duplicates. To accomplish this, let's pass a list of the COUNTRY and YEAR column names into the df.duplicated() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      False\n",
      "1       True\n",
      "2      False\n",
      "3       True\n",
      "4      False\n",
      "       ...  \n",
      "913    False\n",
      "914     True\n",
      "915    False\n",
      "916     True\n",
      "917    False\n",
      "Length: 918, dtype: bool\n"
     ]
    }
   ],
   "source": [
    "combined['COUNTRY'] = combined['COUNTRY'].str.upper()\n",
    "dups = combined.duplicated(['COUNTRY', 'YEAR'])\n",
    "print(dups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correcting Duplicates Values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous screen, we standardized the capitalization of the values in the COUNTRY column and identified that we actually do have three duplicate rows:\n",
    "<br>\n",
    "<br>\n",
    "Let's inspect all the rows for SOMALILAND REGION in combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COUNTRY</th>\n",
       "      <th>DYSTOPIA RESIDUAL</th>\n",
       "      <th>ECONOMY GDP PER CAPITA</th>\n",
       "      <th>FAMILY</th>\n",
       "      <th>FREEDOM</th>\n",
       "      <th>GENEROSITY</th>\n",
       "      <th>HAPPINESS RANK</th>\n",
       "      <th>HAPPINESS SCORE</th>\n",
       "      <th>HEALTH LIFE EXPECTANCY</th>\n",
       "      <th>LOWER CONFIDENCE INTERVAL</th>\n",
       "      <th>STANDARD ERROR</th>\n",
       "      <th>TRUST GOVERNMENT CORRUPTION</th>\n",
       "      <th>UPPER CONFIDENCE INTERVAL</th>\n",
       "      <th>WHISKER HIGH</th>\n",
       "      <th>WHISKER LOW</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>REGION_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>179</td>\n",
       "      <td>SOMALILAND REGION</td>\n",
       "      <td>2.11032</td>\n",
       "      <td>0.18847</td>\n",
       "      <td>0.95152</td>\n",
       "      <td>0.46582</td>\n",
       "      <td>0.50318</td>\n",
       "      <td>91</td>\n",
       "      <td>5.057</td>\n",
       "      <td>0.43873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.06161</td>\n",
       "      <td>0.39928</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015</td>\n",
       "      <td>Sub-Saharan Africa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>498</td>\n",
       "      <td>SOMALILAND REGION</td>\n",
       "      <td>2.43801</td>\n",
       "      <td>0.25558</td>\n",
       "      <td>0.75862</td>\n",
       "      <td>0.39130</td>\n",
       "      <td>0.51479</td>\n",
       "      <td>97</td>\n",
       "      <td>5.057</td>\n",
       "      <td>0.33108</td>\n",
       "      <td>4.934</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.36794</td>\n",
       "      <td>5.18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016</td>\n",
       "      <td>Sub-Saharan Africa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               COUNTRY  DYSTOPIA RESIDUAL  ECONOMY GDP PER CAPITA   FAMILY  \\\n",
       "179  SOMALILAND REGION            2.11032                 0.18847  0.95152   \n",
       "498  SOMALILAND REGION            2.43801                 0.25558  0.75862   \n",
       "\n",
       "     FREEDOM  GENEROSITY  HAPPINESS RANK  HAPPINESS SCORE  \\\n",
       "179  0.46582     0.50318              91            5.057   \n",
       "498  0.39130     0.51479              97            5.057   \n",
       "\n",
       "     HEALTH LIFE EXPECTANCY  LOWER CONFIDENCE INTERVAL  STANDARD ERROR  \\\n",
       "179                 0.43873                        NaN         0.06161   \n",
       "498                 0.33108                      4.934             NaN   \n",
       "\n",
       "     TRUST GOVERNMENT CORRUPTION  UPPER CONFIDENCE INTERVAL  WHISKER HIGH  \\\n",
       "179                      0.39928                        NaN           NaN   \n",
       "498                      0.36794                       5.18           NaN   \n",
       "\n",
       "     WHISKER LOW  YEAR            REGION_y  \n",
       "179          NaN  2015  Sub-Saharan Africa  \n",
       "498          NaN  2016  Sub-Saharan Africa  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined[combined['COUNTRY'] == 'SOMALILAND REGION']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that we shoukd leep the first row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       COUNTRY  DYSTOPIA RESIDUAL  ECONOMY GDP PER CAPITA   FAMILY  FREEDOM  \\\n",
      "0  SWITZERLAND            2.51738                 1.39651  1.34951  0.66557   \n",
      "2      ICELAND            2.70201                 1.30232  1.40223  0.62877   \n",
      "4      DENMARK            2.49204                 1.32548  1.36058  0.64938   \n",
      "6       NORWAY            2.46531                 1.45900  1.33095  0.66973   \n",
      "8       CANADA            2.45176                 1.32629  1.32261  0.63297   \n",
      "\n",
      "   GENEROSITY  HAPPINESS RANK  HAPPINESS SCORE  HEALTH LIFE EXPECTANCY  \\\n",
      "0     0.29678               1            7.587                 0.94143   \n",
      "2     0.43630               2            7.561                 0.94784   \n",
      "4     0.34139               3            7.527                 0.87464   \n",
      "6     0.34699               4            7.522                 0.88521   \n",
      "8     0.45811               5            7.427                 0.90563   \n",
      "\n",
      "   LOWER CONFIDENCE INTERVAL  STANDARD ERROR  TRUST GOVERNMENT CORRUPTION  \\\n",
      "0                        NaN         0.03411                      0.41978   \n",
      "2                        NaN         0.04884                      0.14145   \n",
      "4                        NaN         0.03328                      0.48357   \n",
      "6                        NaN         0.03880                      0.36503   \n",
      "8                        NaN         0.03553                      0.32957   \n",
      "\n",
      "   UPPER CONFIDENCE INTERVAL  WHISKER HIGH  WHISKER LOW  YEAR        REGION_y  \n",
      "0                        NaN           NaN          NaN  2015  Western Europe  \n",
      "2                        NaN           NaN          NaN  2015  Western Europe  \n",
      "4                        NaN           NaN          NaN  2015  Western Europe  \n",
      "6                        NaN           NaN          NaN  2015  Western Europe  \n",
      "8                        NaN           NaN          NaN  2015   North America  \n"
     ]
    }
   ],
   "source": [
    "combined['COUNTRY'] = combined['COUNTRY'].str.upper()\n",
    "combined = combined.drop_duplicates(['COUNTRY', 'YEAR'], keep='first')\n",
    "print(combined.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Missing Values by Dropping Columns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides filling the information, another alternative for missing data is to drop them.\n",
    "<br>\n",
    "<br>\n",
    "From the result above, we can see that some columns have too many missing value that it should not be used. In this case, we should drop it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['LOWER CONFIDENCE INTERVAL' 'STANDARD ERROR' 'UPPER CONFIDENCE INTERVAL'\\n 'WHISKER HIGH' 'WHISKER LOW'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-b70ae2bc612f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mcolumns_to_drop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'LOWER CONFIDENCE INTERVAL'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'STANDARD ERROR'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'UPPER CONFIDENCE INTERVAL'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'WHISKER HIGH'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'WHISKER LOW'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mcombined\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcombined\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns_to_drop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mmissing\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcombined\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4100\u001b[0m             \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4101\u001b[0m             \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4102\u001b[1;33m             \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4103\u001b[0m         )\n\u001b[0;32m   4104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   3912\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3913\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3914\u001b[1;33m                 \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3915\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3916\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[1;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[0;32m   3944\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3945\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3946\u001b[1;33m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3947\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3948\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   5338\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5339\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5340\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{} not found in axis\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5341\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5342\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['LOWER CONFIDENCE INTERVAL' 'STANDARD ERROR' 'UPPER CONFIDENCE INTERVAL'\\n 'WHISKER HIGH' 'WHISKER LOW'] not found in axis\""
     ]
    }
   ],
   "source": [
    "columns_to_drop = ['LOWER CONFIDENCE INTERVAL', 'STANDARD ERROR', 'UPPER CONFIDENCE INTERVAL', 'WHISKER HIGH', 'WHISKER LOW']\n",
    "\n",
    "combined = combined.drop(columns_to_drop, axis=1)\n",
    "missing = combined.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is then deleted \n",
    "<br>\n",
    "<br>\n",
    "As you start working with bigger datasets, it can sometimes be tedious to create a long list of column names to drop. Instead we can use the DataFrame.dropna() method to complete the same task.\n",
    "\n",
    "By default, the dropna() method will drop rows with any missing values. To drop columns, we can set the axis parameter equal to 1, just like with the df.drop() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-a602c760f43a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Missing Values by Dropping Columns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After checking the information, now most of the missing values are fixed.\n",
    "<br>\n",
    "<br>\n",
    "Another alternative to tackle the missing value is imputation i.e ill missing values with reasonable estimates computed from the available data.\n",
    "<br>\n",
    "<br>\n",
    "There are many options for choosing the replacement value, including:\n",
    "<br>\n",
    "<br>\n",
    "A constant value\n",
    "The mean of the column\n",
    "The median of the column\n",
    "The mode of the column\n",
    "First, let's build some intuition around this technique by analyzing how replacing missing values with the mean affects the distribution of the data. In order to do so, we'll use the Series.fillna() method to replace the missing values with the mean.\n",
    "<br>\n",
    "<br>\n",
    "Note that we must pass the replacement value into the Series.fillna() method. For example, if we wanted to replace all of the missing values in the HAPPINESS SCORE column with 0, we'd use the following syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Series.mean of 0      7.587\n",
      "2      7.561\n",
      "4      7.527\n",
      "6      7.522\n",
      "8      7.427\n",
      "       ...  \n",
      "909    3.471\n",
      "911    3.462\n",
      "913    3.349\n",
      "915    2.905\n",
      "917    2.693\n",
      "Name: HAPPINESS SCORE UPDATED, Length: 470, dtype: float64>\n"
     ]
    }
   ],
   "source": [
    "happiness_mean = combined['HAPPINESS SCORE'].mean()\n",
    "combined['HAPPINESS SCORE UPDATED'] = combined['HAPPINESS SCORE'].fillna(happiness_mean)\n",
    "print(combined['HAPPINESS SCORE UPDATED'].mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
